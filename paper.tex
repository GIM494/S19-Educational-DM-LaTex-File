\documentclass[12pt]{article}
\usepackage{amsmath}%
\usepackage{amssymb}%
\usepackage{multicol}
\usepackage{graphicx,epsfig}%
\usepackage[margin=1in]{geometry}
\usepackage{rotating}%
\usepackage{url}
\usepackage[backend=bibtex, citestyle=ieee]{biblatex}
\bibliography{paper}
\usepackage{epsfig}%
\usepackage{epstopdf}%
\usepackage{varwidth}
\usepackage{lscape}%
\usepackage{color}
\usepackage[hang,flushmargin]{footmisc} 
\pdfminorversion 3
\usepackage{pbox}

\newcommand{\hilt}[1]{\colorbox{green}{#1}}
\usepackage{setspace}
\usepackage{adjustbox}
\def\x{\mathbf{x}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand{\tcr}{\textcolor{red}}
\newcommand{\tcb}{\textcolor{blue}}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\DeclareMathOperator*{\argmin}{argmin} 
\renewcommand{\baselinestretch}{1.5}
\usepackage{tabularx}

\usepackage{bibentry}

\title{S19: REU Research Document}
\author{Emily Slaughter, Gennie Mansi}

\begin{document}

\maketitle

\clearpage

\tableofcontents

\clearpage

%%%%%%%%%%%%%%%%%%%
\section{Abstract} \label{abstract}



\clearpage
%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{intro}

Proper documentation forms an integral part of developing and maintaining software, especially given the increasing size of code bases and the prevalence of temporally and spatially dispersed teams. As code is written and the project develops, various design decisions are made, which impact both existing and future code: ideally, as code is authored or modified, it should follow design rules used to author prior code. A project's \textit{design rules} are the set of choices about the code's design and their implementation. Documentation of design rules and the rationale behind them is dependent on the developer, and when it is recorded, it is often dispersed among several disparate locations, including a formal document, comments in the code, and the developers themselves. However, documentation is often not written, read, or updated, and coworkers are not always a reliable resource \cite{KoEtAl2007}. Lack of documentation ultimately causes newcomers to a project to depend largely on the code rather than explicit, written documentation to understand such project-specific design rules.

A tool that mines and recommends design rules has the potential to facilitate active documentation of code, allowing developers to efficiently and accurately apply and maintain a unified set of design rules. There are an exponential number of patterns that could be found in a given code base, and some rules have greater relevance than others. Finding an excessive number of non-existent rules or missing important rules would discourage developers from using the tool to document their code. Hence our focus is to intelligently and efficiently determine and interpret the patterns in a codebase that are most indicative and representative of meaningful design rules.

Several sources of information in addition to the code itself, including search history, cursor location, event logs, and debugger information can indicate what elements in the code base are most pertinent to the user and are most likely to contain design rules of interest. We explore how this information from outside of the code base can be combined with the code base itself to inform what patterns are identified. Ultimately, such a tool will greatly increase the amount of up-to-date documentation available to developers. We seek to develop a tool that automates the manual rule-authoring capabilities of ActiveDocumentation \cite{MehrpurEtAl2019} and preserves its live-update features.

\tcr{Note the following paragraph (surrounded by red brackets) is a rough outline of what our process will be; it is vague and most certainly will change: [ }Our approach to suggesting rules is threefold. First, we identify and record the presence of key features in the code base in a binary database that associates the features with specific sections of the code. Second, we use association rule mining to associate these key features together. Finally, we use the associated key features to construct the most likely rules, which we present to the user for acceptance or modification. \tcr{]}
 
 We structure the rest of this paper as follows: In Section~\ref{probDesc}, we define our goal within the context of association rule mining, and we describe our approach given the assumptions we are making about the user. In Section~\ref{toolInterface}, we describe the general structure of the tool interface we would like to implement. Finally, in Section~\ref{relatedWorks} and Section~\ref{disc} we present related works and discuss the efficiency and limitations of our implementation.
 \clearpage


%%%%%%%%%%%%%%%%%%%
\section{Methods}\label{methods}

\subsection{Problem Description} \label{probDesc}

Given the code for a project and some external information such as cursor information, we wish to derive a set of most likely and relevant design rules. We will then integrate our algorithm for mining these design rules into an interface that allows developers to seamlessly mine and write design rules and to see violations and correct examples of these rules. 

When considering our task, there exist two significant considerations: (a) what kinds of data are available and helpful for suggesting rules, and (b) what kind of algorithm can be utilized with these sources of data in order to make relevant design rule suggestions. We will begin by first addressing different sources of information that are available and most useful for our purpose. Then, we will proceed to discuss our association rule mining algorithm for identifying code patterns and synthesizing design rules.

\subsection{Assumptions about the user} \label{assumptions}

Developers can be divided into three categories with respect to their familiarity with a project or code base. First, a developer may be experienced with a given code base. She has a thorough understanding of the project, its objectives, and its implementation. She is in a position that allows her to author design rules and verify that certain rules have been followed through the project. Second, a developer may be partially knowledgable about a project and its implementation. Such a developer may have a general familiarity with the code and its implementations, but is not able to easily or confidently author design rules or may simply wish to explore possible existing design rules. On her own, she is not be able to produce an accurate rule, but might be able to see a rule and provide useful feedback on its correctness or relevancy. Finally, a developer may be completely unfamiliar with a code base. Existing design rules and documents would be extremely helpful in helping her understand the structure of the code. She may also need to frequently consult co-workers about code standards and patterns if there is a significant lack of written documentation. In such a situation, the developer may even be interested in looking for examples on what kind of existing design rules to which she should adhere.

Work has already been completed with regards to the first category of developers, and while much research has been done in code suggestion, completely automated design rule generation seems largely unexplored (see Section~\ref{relatedWorks}). Therefore, an algorithm developed with the second category of developers in mind forms a natural bridge between human-authored design rules and completely automated rule authoring. Consequently, we assume the developers for which we are designing our algorithm have some general familiarity with the code and can identify proper design rules if supplied with a set of rules to consider. Thus, feedback form the developer about hypothesized rules is dependable, enabling mixed human-AI rule authoring.

\subsection{Sources of Information} \label{infoSrcs}

Part of knowing which sources are most useful for deriving information about design rules is understanding where developers themselves look for information about a project and what kinds of information are useful to programmers.  

\subsection{Possible sources}

Several studies have demonstrated that as developers work and explore a project, they try to answer high-level questions about what, why, and how code causes different program states as they try to complete a task \cite{SadowskiEtAl2015, LaTozaMyers2010, LaTozaEtAl2007}. Researchers who investigated how programmers read and explore code have found that programmers' activities include checking implementation details, checking common style, and browsing. Programmers also performed code search to understand various dependencies within and between projects, why something is failing, and the side effects of a proposed changed \cite{SadowskiEtAl2015}. 

Ko et al investigate what sources of information developers use to accomplish a task and how that information affects developers' workflow throughout the day. They report that developers look at a variety of sources to accomplish a task, including check-in logs, bug reports, content management systems, version control systems, and other coworkers. Additionally, they identify knowledge about design and program behavior as the type of information that is most often deferred since unavailable coworkers are often the only source of knowledge. Developers sought out coworkers most often when there are information needs with regards to code design. Other questions to coworkers centered around following the team's conventions. Developers asked specific questions regarding what code caused different program states and behaviors and what statically related to the code of interest, but pursued the answers to these questions by starting with a hypothesis that they largely developed by using their intuition, asking coworkers, looking for execution logs, scouring bug reports, and using the debugger. They then often refined their hypothesis by using the search tool to answer questions about which sections of code performed similar operations \cite{KoEtAl2007}. 

Furthermore, Fritz et al conducted a survey of nineteen professional Java programmers in order to generate a degree-of-interest (DOI) model to reflect developers' knowledge of code \cite{FritzEtAl2007}. The researchers' model was based on the total number of interactions the programmer has with an element and the recency of that interaction. Other factors they identified that could be used to improve their model included authorship of program elements, the role of elements, and the task being performed \cite{FritzEtAl2007}.

Based off of  these findings, we identified the debugger (?), code base, cursor location and search histories, and the developer herself as the most pertinent and accessible sources of information for our algorithm.

\textit{Debugger.} [Insert something later when we know how exactly we will be using the debugger as a source of information ]

\textit{Cursor location.} [Insert something later when we know how exactly we will be using the cursor location as a source of information ]

\textit{Search history.} [Insert something later when we know how exactly we will be using the search history as a source of information ]

\textit{Developer.} [Insert something later when we know how exactly we will be using the developer as a source of information ]

\textit{Code base.} [Insert something later when we know how exactly we will be using the code base as a source of information ]



\subsection{Algorithm} \label{algm}

We need an efficient and compact way to represent the sundry features of a codebase and of identifying which features commonly appear together. The field of association rule mining is recognized for its ability to discover sets of items that co-occur with high prevalence without much previous information about the population or features \cite{AiEtAl2018}. Given that we are trying to associate a set of features for a codebase without any dependable knowledge about which features are most likely to be most common, association rule mining presents useful and powerful tools. There are several challenges that we encounter. First we have a large number of features, especially in comparison to the number of files in which those features are found, so our chosen algorithm must work with high dimensional data. Second, code bases are active documents; they are constantly changing and updating, and their features are constantly changing as well, so our algorithm must work with incremental data. Finally, many association rule mining algorithms are recursive, but the data structure used to compact and mine the key features can become so deep that a recursive algorithm can cause a stack overflow error, so our algorithm should be iterative. Therefore, we need an incremental, non-recrusive association rule miner that can efficiently analyze high dimensional data. In the following subsections, we begin by establishing the notation we will use for the rest of the paper; we discuss how we filter, aggregate, and update the database used with the association rule miner; we discuss the details of the algorithm we implemented and how we generate queries using the key features we association. Finally, we discuss how queries are selected and presented to the user.

\subsubsection{Notation}

In association rule mining, the database, $D$, from which the rules are mined are composed of features (columns) and transactions (rows). The set of features, more commonly referred to as \textit{items} are what we wish to find associations between. Each transaction, represents an instance of the dataset that contain certain features. In our case, each file is a transaction that contains several code features which we wish to associate in order to derive rules. Let $I=\{i_1, i_2, \ldots, i_n\}$ be the set of all $n$ features that are present in the code base that we use to identify patterns, and let $T$ be a transaction, which is a subset of $I$. An association rule is an inference between two subsets, $X$ and $Y$, such that $X \rightarrow Y$ and $X \cap Y = \emptyset$. In association rule mining, $X$ is referred to as the \textit{antecedent} and $Y$ is known as the \textit{consequent}. The \textit{frequency} is the number of rows that contain $X$; the \textit{support} is the fraction of all rows that contain the itemset. The support of $X \rightarrow Y$ is the union of $X$ and $Y$. The confidence of $X \rightarrow Y$ is the conditional probability of $Y$ given $X$. In association rule mining, an itemset is considered \textit{frequent} if its support is greater than or equal to a user-specified minimum support threshold. 

\subsubsection{Filtering, aggregating, and updating information}

\subsubsection{Identifying code patterns}

\subsubsection{Generating queries}


\subsubsection{Selecting queries to present to user}

\clearpage

%%%%%%%%%%%%%%%%%%%

\section{Tool interface} \label{toolInterface}
There are several major considerations even when we narrow our focus to a developer in the second situation. These considerations divide generally into considerations of what kinds of data are available and helpful for suggesting rules and how these kinds of data can be utilized to make relevant design rule suggestions.

There are sundry sources of data available that can and do provide information about the code base and what sections of the code in which the developer might be most interested. The most obvious source of information about the code is the code itself. Using a tool called srcML, the code can be put in a specific XML format that can then be searched using XPaths. Thus we can identify all kinds of information about the code itself, including function and class names and declarations, member variables, and relationships between classes like child and parent classes. Information is also available from the IDE itself. Such information can include the cursor's current and previous location, the user's search history, and which file windows are currently open in the project. Finally, the tool itself could be a possible source of information. For example, information about existing rules may be used to help improve the relevancy of the design rules presented to the developer.

There are a couple of approaches that could be taken with respect to different approaches to ensure relevant design rule suggestions. One way of viewing this challenge is to view it as a source of natural language programming (NLP) problem. One group of researchers were trying to improve code suggestions and viewed their challenge as analogous to a more traditional NLP problem of trying to fill in a missing word in a sentence. They developed a statistical language model to aid in providing suggestions for different code snippets \cite{RaychevEtAl2014}. We could think of our challenge as analogous to an NLP problem that aims to outline to a corpus of text or maybe, even more abstractly, find rhetorical devices in a text. Alternatively, we could simply rely more on traditional papers and research that have been conducted in this area that do not rely on NLP techniques to provide solutions. Both perspective may prove useful in the initial stages of research that entail exploring different ways that information is stored and what information about developer activity has seemed most helpful in designing related tools. 



\clearpage

%%%%%%%%%%%%%%%%%%%

\section{Related works}\label{relatedWorks}

Many tools and methods have been devised to augment programmers' ability to accomplish a task including code recommender systems for novices \cite{HartmannEtAl2010}, search tools for identifying bugs \cite{HovemeyerPugh2004}, search tools for identifying relevant API functions and the sections of code that need to be altered \cite{RongEtAl2016}, and code suggestion \cite{RaychevEtAl2014}. However, one area that seems largely unexplored is tools that facilitate the documentation of code.

Through what is called implicit group memory, Hipikat is a client-server system that automates the mentor-like relationship that develops between developers who are new to a project and senior developers who watch over the newcomers and pass down knowledge. Hipikat learn from four types of artifacts and communications stored in a project's history: bug and feature descriptions, source file revisions, messages posted on developer forums, and other project documents. Artifacts may be queried, and the artifact database may be searched \cite{CubranicMurphy2003}.

LaToza et al explored finding design patterns in an HTML document in order to perform code prediction \cite{LaTozaEtAl2019}. However, their tool does not identify design rules that might explain that pattern. 
ActiveDocumentation is a plug-in for IntelliJ IDEA that can be used to author rules and to view correct examples and violations of rules in a given code base in real time \cite{MehrpurEtAl2019}. The ActiveDocumentation plug-in helps the developer author, check, and fix rules, efficiently and in real time. It automatically updates its interface with correct examples and violations of design rules written in the program \cite{MehrpurEtAl2019}. Neither of these tools, however, integrates information from sources outside of the code base such as cursor information or search history. Mylar is a tool that exists within the Eclipse IDE and encodes a programmer's activity with code into a DOI model that can then be used to highlight information in a program that may be related to the task that a developer is completing. This model focuses on individual program elements without taking into account the possible structural relationships that may exist between program elements \cite{KerstenMurphy2005}.

\bigskip

\tcr{Imagine some bridge between the two of these or maybe the following section gets shifted up to the algorithm section??}

A naive approach, known as the Apriori approach, is to generate all candidate sets and then eliminate candidate sets whose support is less than the minimum support threshold. Depending on the number of  items, this can grow computationally prohibitive, so many authors look for ways to eliminate candidate set generation and to minimize the number of times the database needs to be scanned in order to produce frequent itemsets and association rules \cite{NarvekarSyed2015, WeiEtAl2014, EzeifeSu2002, LiEtAl2006}. 

\tcr{Continued discussion of FP mining; I'm holding off on inserting this into the document until we are more certain about the algorithm we wish to use, just so that we only pick the most related works.}




\bigskip


\clearpage

%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{disc}

It might be helpful to adapt our tool to be able to work with team environments. 

I don't really feel like discussing things any more. Why is this the only part that is called the discussion? I mean, what has been the rest of everything that has been written? Or did you just skip here? Is that why it has been called ``discussion"?

\clearpage

%%%%%%%%%%%%%%%%%%%%
\section{References}\label{references}

\printbibliography

\end{document}
